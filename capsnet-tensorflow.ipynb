{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-3c2ea306bb97>:3: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /home/jose/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /home/jose/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jose/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /home/jose/.local/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When you re-run the notebook without restarting the kernel\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To produce the same output\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    sample_image = mnist.train.images[index].reshape(28, 28)\n",
    "    plt.imshow(sample_image, cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 4, 6, 1], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train.labels[:n_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape=[None, 28, 28, 1], dtype=tf.float32, name=\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Capsules\n",
    "caps1_n_maps = 32\n",
    "caps1_n_caps = caps1_n_maps * 6 * 6  # 1152 primary capsules\n",
    "caps1_n_dims = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_params = {\n",
    "    # Conv1 has 256, 9x9 convolution kernels with a stride of 1 and ReLU activation\n",
    "    \"filters\": 256,\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 1,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu,\n",
    "}\n",
    "\n",
    "conv2_params = {\n",
    "    #  The second layer (PrimaryCapsules) is a convolutional capsule layer with 32\n",
    "    #  channels of covolutional 8D capsules.\n",
    "    #\n",
    "    #  Each primary capsules contains 8 convolutional units with 9x9 kernels\n",
    "    #  and a stride of 2.\n",
    "    \n",
    "    \"filters\": caps1_n_maps * caps1_n_dims, # 256 convolutional filters\n",
    "    \"kernel_size\": 9,\n",
    "    \"strides\": 2,\n",
    "    \"padding\": \"valid\",\n",
    "    \"activation\": tf.nn.relu\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape :  (?, 20, 20, 256)\n",
      "conv2 shape :  (?, 6, 6, 256)\n"
     ]
    }
   ],
   "source": [
    "conv1 = tf.layers.conv2d(inputs = X    , name = \"conv1\", **conv1_params)\n",
    "conv2 = tf.layers.conv2d(inputs = conv1, name = \"conv2\", **conv2_params)\n",
    "\n",
    "print(\"conv1 shape : \", conv1.shape) # 20x20x256\n",
    "print(\"conv2 shape : \", conv2.shape) # 6x6x(8x32)\n",
    "\n",
    "# PrimaryCapsules has [32x6x6] capsule outputs(each output is an 8D vector) and\n",
    "# each capsule in the [6x6] grid is sharing their weights with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_raw shape :  (?, 1152, 8)\n"
     ]
    }
   ],
   "source": [
    "conv1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims], name = \"caps1_raw\")\n",
    "\n",
    "print(\"conv1_raw shape : \", conv1_raw.shape) # (6x6x32)x8, 8 = caps1_n_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s, axis = -1, epsilon = 1e-7, name = None):\n",
    "    with tf.name_scope(name, default_name = \"squash\"):\n",
    "        squared_norm  = tf.reduce_sum(tf.square(s), axis = axis, keep_dims = True)\n",
    "        safe_norm     = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1.0 + squared_norm)\n",
    "        \n",
    "        return squash_factor * s / safe_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-12-18ceecb26120>:3: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "caps1_output shape :  (?, 1152, 8)\n"
     ]
    }
   ],
   "source": [
    "caps1_output = squash(conv1_raw, name = \"caps1_output\")\n",
    "\n",
    "print(\"caps1_output shape : \", caps1_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit Capsules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_n_caps = 10\n",
    "caps2_n_dims = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caps1_output_expanded shape :  (?, 1152, 8, 1)\n",
      "caps1_output_tile shape :  (?, 1152, 1, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "caps1_output_expanded = tf.expand_dims(caps1_output, -1,\n",
    "                                       name=\"caps1_output_expanded\")\n",
    "\n",
    "print(\"caps1_output_expanded shape : \", caps1_output_expanded.shape)\n",
    "\n",
    "caps1_output_tile     = tf.expand_dims(caps1_output_expanded, 2,\n",
    "                                   name=\"caps1_output_tile\")\n",
    "\n",
    "print(\"caps1_output_tile shape : \", caps1_output_tile.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W shape:  (1, 1152, 160, 8, 1)\n",
      "biases shape:  (1, 1, 10, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "init_sigma = 0.1\n",
    "\n",
    "W_init = tf.random_normal(\n",
    "                    shape=(1, caps1_output_tile.shape[ 1].value, caps2_n_caps * caps2_n_dims, \n",
    "                              caps1_output_tile.shape[-2].value, caps1_output_tile.shape[-1].value),\n",
    "                    stddev=init_sigma, dtype=tf.float32, name=\"W_init\")\n",
    "\n",
    "W      = tf.Variable(W_init, name=\"W\")\n",
    "\n",
    "biases = tf.get_variable('bias', shape = (1 , 1, caps2_n_caps, caps2_n_dims, 1))\n",
    "\n",
    "print(\"W shape: \", W.shape)\n",
    "print(\"biases shape: \", biases.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caps1_output_tiled :  (?, 1152, 160, 8, 1)\n"
     ]
    }
   ],
   "source": [
    "caps1_output_tiled    = tf.tile(caps1_output_tile, [1, 1, caps2_n_caps * caps2_n_dims, 1, 1],\n",
    "                             name=\"caps1_output_tiled\")\n",
    "\n",
    "print(\"caps1_output_tiled : \", caps1_output_tiled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat shape:  (?, 1152, 160, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "u_hat = tf.reduce_sum(W * caps1_output_tiled, axis = 3, keep_dims = True)\n",
    "\n",
    "print(\"u_hat shape: \", u_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_hat shape:  (?, 1152, 10, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "u_hat = tf.reshape(u_hat, shape = [-1, u_hat.shape[1].value, caps2_n_caps, caps2_n_dims, 1])\n",
    "\n",
    "print(\"u_hat shape: \", u_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_hat_stopped = tf.stop_gradient(u_hat, name = 'stop_gradient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b_IJ.shape =  (?, 1152, 10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "b_IJ = tf.zeros([batch_size, caps1_n_caps, caps2_n_caps, 1, 1], dtype = tf.float32, name = 'b_IJ')\n",
    "\n",
    "print(\"b_IJ.shape = \", b_IJ.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Routing by agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v_J_tiled.shape =  (?, 1152, 10, 16, 1)\n",
      "u_produce_v.shape =  (?, 1152, 10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "rounds = 2\n",
    "\n",
    "for r_iter in range(rounds):\n",
    "    with tf.variable_scope('iter_' + str(r_iter)):\n",
    "        c_IJ = tf.nn.softmax(b_IJ, axis=2)\n",
    "        \n",
    "        if r_iter == rounds - 1:\n",
    "            s_J = tf.multiply(c_IJ, u_hat)\n",
    "            s_J = tf.reduce_sum(s_J, axis = 1, keepdims = True) + biases\n",
    "            v_J = squash(s_J) \n",
    "            \n",
    "        elif r_iter < rounds - 1:\n",
    "            s_J = tf.multiply(b_IJ, u_hat_stopped)\n",
    "            s_J = tf.reduce_sum(s_J, axis = 1, keepdims = True) + biases\n",
    "            \n",
    "            v_J = squash(s_J)\n",
    "            \n",
    "            v_J_tiled   = tf.tile(v_J, [1, u_hat.shape[1].value, 1, 1, 1])\n",
    "            print(\"v_J_tiled.shape = \", v_J_tiled.shape)\n",
    "            u_produce_v = tf.reduce_sum(u_hat_stopped * v_J_tiled, axis = 3, keepdims = True)\n",
    "            \n",
    "            print(\"u_produce_v.shape = \", u_produce_v.shape)\n",
    "            b_IJ += u_produce_v\n",
    "\n",
    "            \n",
    "caps2_output = v_J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328350, 100)\n"
     ]
    }
   ],
   "source": [
    "def condition(input, counter):\n",
    "    return tf.less(counter, 100)\n",
    "\n",
    "def loop_body(input, counter):\n",
    "    output = tf.add(input, tf.square(counter))\n",
    "    return output, tf.add(counter, 1)\n",
    "\n",
    "with tf.name_scope(\"compute_sum_of_squares\"):\n",
    "    counter = tf.constant(1)\n",
    "    sum_of_squares = tf.constant(0)\n",
    "\n",
    "    result = tf.while_loop(condition, loop_body, [sum_of_squares, counter])\n",
    "    \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "338350"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([i**2 for i in range(1, 100 + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimated Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caps2_output shape :  (?, 1, 10, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"caps2_output shape : \", caps2_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_norm(s, axis=-1, epsilon=1e-7, keep_dims=False, name=None):\n",
    "    with tf.name_scope(name, default_name=\"safe_norm\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=keep_dims)\n",
    "        return tf.sqrt(squared_norm + epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_proba shape :  (?, 1, 10, 1)\n"
     ]
    }
   ],
   "source": [
    "y_proba = safe_norm(caps2_output, axis=-2, name=\"y_proba\")\n",
    "\n",
    "print(\"y_proba shape : \", y_proba.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba_argmax = tf.argmax(y_proba, axis=2, name=\"y_proba\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_proba_argmax shape :  (?, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_proba_argmax shape : \", y_proba_argmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.squeeze(y_proba_argmax, axis=[1,2], name=\"y_pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "\n",
    "y = tf.placeholder(shape=[None], dtype=tf.int64, name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Margin Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_plus = 0.9\n",
    "m_minus = 0.1\n",
    "lambda_ = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T shape :  (?, 10)\n"
     ]
    }
   ],
   "source": [
    "T = tf.one_hot(y, depth=caps2_n_caps, name=\"T\")\n",
    "\n",
    "print(\"T shape : \", T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session():\n",
    "    print(T.eval(feed_dict={y: np.array([0, 1, 2, 3, 9])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caps2_output_norm shape :  (?, 1, 10, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "caps2_output_norm = safe_norm(caps2_output, axis=-2, keep_dims=True,\n",
    "                              name=\"caps2_output_norm\")\n",
    "\n",
    "print(\"caps2_output_norm shape : \", caps2_output_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "present_error_raw shape :  (?, 1, 10, 1, 1)\n",
      "present_error shape :  (?, 10)\n"
     ]
    }
   ],
   "source": [
    "present_error_raw = tf.square(tf.maximum(0., m_plus - caps2_output_norm),\n",
    "                              name=\"present_error_raw\")\n",
    "\n",
    "print(\"present_error_raw shape : \", present_error_raw.shape)\n",
    "\n",
    "present_error = tf.reshape(present_error_raw, shape=(-1, 10),\n",
    "                           name=\"present_error\")\n",
    "\n",
    "print(\"present_error shape : \", present_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absent_error_raw shape :  (?, 1, 10, 1, 1)\n",
      "absent_error shape :  (?, 10)\n"
     ]
    }
   ],
   "source": [
    "absent_error_raw = tf.square(tf.maximum(0., caps2_output_norm - m_minus),\n",
    "                             name=\"absent_error_raw\")\n",
    "\n",
    "print(\"absent_error_raw shape : \", absent_error_raw.shape)\n",
    "\n",
    "absent_error = tf.reshape(absent_error_raw, shape=(-1, 10),\n",
    "                          name=\"absent_error\")\n",
    "\n",
    "print(\"absent_error shape : \", absent_error.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L shape :  (?, 10)\n"
     ]
    }
   ],
   "source": [
    "L = tf.add(T * present_error, lambda_ * (1.0 - T) * absent_error,\n",
    "           name=\"L\")\n",
    "\n",
    "print(\"L shape : \", L.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "margin_loss shape :  ()\n"
     ]
    }
   ],
   "source": [
    "# Margin Loss\n",
    "\n",
    "margin_loss = tf.reduce_mean(tf.reduce_sum(L, axis=1), name=\"margin_loss\")\n",
    "\n",
    "print(\"margin_loss shape : \", margin_loss.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_with_labels = tf.placeholder_with_default(False, shape=(),\n",
    "                                               name=\"mask_with_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_targets = tf.cond(mask_with_labels, # condition\n",
    "                                 lambda: y,        # if True\n",
    "                                 lambda: y_pred,   # if False\n",
    "                                 name=\"reconstruction_targets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_mask = tf.one_hot(reconstruction_targets,\n",
    "                                 depth=caps2_n_caps,\n",
    "                                 name=\"reconstruction_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reconstruction_mask:0' shape=(?, 10) dtype=float32>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'iter_1/squash/truediv_1:0' shape=(?, 1, 10, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_mask_reshaped = tf.reshape(\n",
    "    reconstruction_mask, [-1, 1, caps2_n_caps, 1, 1],\n",
    "    name=\"reconstruction_mask_reshaped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'reconstruction_mask_reshaped:0' shape=(?, 1, 10, 1, 1) dtype=float32>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_mask_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps2_output_masked = tf.multiply(\n",
    "    caps2_output, reconstruction_mask_reshaped,\n",
    "    name=\"caps2_output_masked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'caps2_output_masked:0' shape=(?, 1, 10, 16, 1) dtype=float32>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caps2_output_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = tf.reshape(caps2_output_masked,\n",
    "                           [-1, caps2_n_caps * caps2_n_dims],\n",
    "                           name=\"decoder_input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'decoder_input:0' shape=(?, 160) dtype=float32>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden1 = 512\n",
    "n_hidden2 = 1024\n",
    "n_output = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"decoder\"):\n",
    "    hidden1 = tf.layers.dense(decoder_input, n_hidden1,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2,\n",
    "                              activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    decoder_output = tf.layers.dense(hidden2, n_output,\n",
    "                                     activation=tf.nn.sigmoid,\n",
    "                                     name=\"decoder_output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flat = tf.reshape(X, [-1, n_output], name=\"X_flat\")\n",
    "squared_difference = tf.square(X_flat - decoder_output,\n",
    "                               name=\"squared_difference\")\n",
    "reconstruction_loss = tf.reduce_mean(squared_difference,\n",
    "                                    name=\"reconstruction_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0005\n",
    "#alpha = 0.0\n",
    "loss = tf.add(margin_loss, alpha * reconstruction_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.equal(y, y_pred, name=\"correct\")\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Val accuracy: 98.9600%  Loss: 0.013421 (improved)\n",
      "Epoch: 2  Val accuracy: 99.2200%  Loss: 0.008745 (improved)\n",
      "Epoch: 3  Val accuracy: 99.4600%  Loss: 0.008114 (improved)\n",
      "Epoch: 4  Val accuracy: 99.4000%  Loss: 0.007337 (improved)\n",
      "Epoch: 5  Val accuracy: 99.2600%  Loss: 0.007228 (improved)\n",
      "Epoch: 6  Val accuracy: 99.4200%  Loss: 0.006432 (improved)\n",
      "Epoch: 7  Val accuracy: 99.4400%  Loss: 0.007008\n",
      "Epoch: 8  Val accuracy: 99.5600%  Loss: 0.006214 (improved)\n",
      "Epoch: 9  Val accuracy: 99.4600%  Loss: 0.007288\n",
      "Epoch: 10  Val accuracy: 99.4600%  Loss: 0.007174\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 50\n",
    "restore_checkpoint = False\n",
    "\n",
    "n_iterations_per_epoch = mnist.train.num_examples // batch_size\n",
    "n_iterations_validation = mnist.validation.num_examples // batch_size\n",
    "best_loss_val = np.infty\n",
    "checkpoint_path = \"./my_capsule_network\"\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    if restore_checkpoint and tf.train.checkpoint_exists(checkpoint_path):\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "    else:\n",
    "        init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(1, n_iterations_per_epoch + 1):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            # Run the training operation and measure the loss:\n",
    "            _, loss_train = sess.run(\n",
    "                [training_op, loss],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch,\n",
    "                           mask_with_labels: True})\n",
    "            print(\"\\rIteration: {}/{} ({:.1f}%)  Loss: {:.5f}\".format(\n",
    "                      iteration, n_iterations_per_epoch,\n",
    "                      iteration * 100 / n_iterations_per_epoch,\n",
    "                      loss_train),\n",
    "                  end=\"\")\n",
    "\n",
    "        # At the end of each epoch,\n",
    "        # measure the validation loss and accuracy:\n",
    "        loss_vals = []\n",
    "        acc_vals = []\n",
    "        for iteration in range(1, n_iterations_validation + 1):\n",
    "            X_batch, y_batch = mnist.validation.next_batch(batch_size)\n",
    "            loss_val, acc_val = sess.run(\n",
    "                    [loss, accuracy],\n",
    "                    feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                               y: y_batch})\n",
    "            loss_vals.append(loss_val)\n",
    "            acc_vals.append(acc_val)\n",
    "            print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                      iteration, n_iterations_validation,\n",
    "                      iteration * 100 / n_iterations_validation),\n",
    "                  end=\" \" * 10)\n",
    "        loss_val = np.mean(loss_vals)\n",
    "        acc_val = np.mean(acc_vals)\n",
    "        print(\"\\rEpoch: {}  Val accuracy: {:.4f}%  Loss: {:.6f}{}\".format(\n",
    "            epoch + 1, acc_val * 100, loss_val,\n",
    "            \" (improved)\" if loss_val < best_loss_val else \"\"))\n",
    "\n",
    "        # And save the model if it improved:\n",
    "        if loss_val < best_loss_val:\n",
    "            save_path = saver.save(sess, checkpoint_path)\n",
    "            best_loss_val = loss_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network\n",
      "Final test accuracy: 99.4500%  Loss: 0.006010   \n"
     ]
    }
   ],
   "source": [
    "n_iterations_test = mnist.test.num_examples // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "\n",
    "    loss_tests = []\n",
    "    acc_tests = []\n",
    "    for iteration in range(1, n_iterations_test + 1):\n",
    "        X_batch, y_batch = mnist.test.next_batch(batch_size)\n",
    "        loss_test, acc_test = sess.run(\n",
    "                [loss, accuracy],\n",
    "                feed_dict={X: X_batch.reshape([-1, 28, 28, 1]),\n",
    "                           y: y_batch})\n",
    "        loss_tests.append(loss_test)\n",
    "        acc_tests.append(acc_test)\n",
    "        print(\"\\rEvaluating the model: {}/{} ({:.1f}%)\".format(\n",
    "                  iteration, n_iterations_test,\n",
    "                  iteration * 100 / n_iterations_test),\n",
    "              end=\" \" * 10)\n",
    "    loss_test = np.mean(loss_tests)\n",
    "    acc_test = np.mean(acc_tests)\n",
    "    print(\"\\rFinal test accuracy: {:.4f}%  Loss: {:.6f}\".format(\n",
    "        acc_test * 100, loss_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_capsule_network\n"
     ]
    }
   ],
   "source": [
    "n_samples = 5\n",
    "\n",
    "sample_images = mnist.test.images[:n_samples].reshape([-1, 28, 28, 1])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, checkpoint_path)\n",
    "    caps2_output_value, decoder_output_value, y_pred_value = sess.run(\n",
    "            [caps2_output, decoder_output, y_pred],\n",
    "            feed_dict={X: sample_images,\n",
    "                       y: np.array([], dtype=np.int64)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_images = sample_images.reshape(-1, 28, 28)\n",
    "reconstructions = decoder_output_value.reshape([-1, 28, 28])\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.imshow(sample_images[index], cmap=\"binary\")\n",
    "    plt.title(\"Label:\" + str(mnist.test.labels[index]))\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(n_samples * 2, 3))\n",
    "for index in range(n_samples):\n",
    "    plt.subplot(1, n_samples, index + 1)\n",
    "    plt.title(\"Predicted:\" + str(y_pred_value[index]))\n",
    "    plt.imshow(reconstructions[index], cmap=\"binary\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
